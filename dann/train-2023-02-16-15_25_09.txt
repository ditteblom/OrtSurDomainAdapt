Finished loading dataset.
Finished loading dataset.
Finished loading dataset.
Data loaded. Start training...
/zhome/f6/e/127187/miniconda3/envs/py39/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
lr [0.010000000000000002, 0.1, 0.1, 0.1]
Epoch: [0/100], Regression loss: 0.17723409831523895, Transfer loss: 0.6816083788871765, Total loss: 0.8588424921035767, Domain acc.: 58.33333206176758

Traceback (most recent call last):
  File "/work3/dgro/OrtSurDomainAdapt/main.py", line 164, in <module>
    main(args)
  File "/work3/dgro/OrtSurDomainAdapt/main.py", line 100, in main
    (val_loss, val_acc) = validate(val_loader, regressor, dann)
  File "/work3/dgro/OrtSurDomainAdapt/utils/analysis/validate.py", line 19, in validate
    for i, (images, scores) in enumerate(val_loader):
  File "/zhome/f6/e/127187/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 517, in __next__
    data = self._next_data()
  File "/zhome/f6/e/127187/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 557, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/zhome/f6/e/127187/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/zhome/f6/e/127187/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py", line 83, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/zhome/f6/e/127187/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py", line 83, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/zhome/f6/e/127187/miniconda3/envs/py39/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py", line 55, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: All input tensors must be on the same device. Received cuda:0 and cpu
